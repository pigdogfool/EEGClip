import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
import numpy as np
import pandas as pd
import os
from tqdm import tqdm
import matplotlib.pyplot as plt
import logging
import json
from datetime import datetime
from pathlib import Path

from eeg_to_text_model_contrastive import ContrastiveEEGTextModel

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class EEGTextDatasetFullTrain(Dataset):
    """EEGå’Œæ–‡æœ¬é…å¯¹æ•°æ®é›† - å…¨éƒ¨ç”¨äºè®­ç»ƒ"""
    
    def __init__(self, csv_file, eeg_dir, tokenizer, max_text_length=32):
        self.data = pd.read_csv(csv_file)
        self.eeg_dir = eeg_dir
        self.tokenizer = tokenizer
        self.max_text_length = max_text_length
        
        # è¿‡æ»¤æ‰æ²¡æœ‰å¯¹åº”EEGæ–‡ä»¶çš„æ•°æ®
        self.valid_data = []
        not_found_ids = []
        
        for idx, row in self.data.iterrows():
            possible_patterns = [
                f"preprocessed_{row['id']}_raw_300_0328.npy",
                f"preprocessed_{row['id']}.npy",
                f"{row['id']}_raw_300_0328.npy",
                f"{row['id']}.npy",
            ]
            
            eeg_path = None
            for pattern in possible_patterns:
                test_path = os.path.join(eeg_dir, pattern)
                if os.path.exists(test_path):
                    eeg_path = test_path
                    break
            
            if eeg_path:
                self.valid_data.append({
                    'id': row['id'],
                    'sentence': row['sentence'],
                    'eeg_path': eeg_path
                })
            else:
                not_found_ids.append(row['id'])
        
        logger.info(f"âœ… æ‰¾åˆ° {len(self.valid_data)} ä¸ªæœ‰æ•ˆçš„EEG-æ–‡æœ¬é…å¯¹ç”¨äºé¢„è®­ç»ƒ")
        if len(not_found_ids) > 0:
            logger.warning(f"âš ï¸ æœªæ‰¾åˆ°EEGæ–‡ä»¶çš„IDæ•°é‡: {len(not_found_ids)}")
            logger.warning(f"   å‰10ä¸ªæœªæ‰¾åˆ°çš„ID: {not_found_ids[:10]}")
    
    def __len__(self):
        return len(self.valid_data)
    
    def __getitem__(self, idx):
        item = self.valid_data[idx]
        
        # åŠ è½½EEGæ•°æ®
        try:
            eeg_data = np.load(item['eeg_path'])
            eeg_data = eeg_data.astype(np.float32)
            
            # æ ‡å‡†åŒ–æ¯ä¸ªé€šé“
            for ch in range(eeg_data.shape[0]):
                mean = np.mean(eeg_data[ch])
                std = np.std(eeg_data[ch])
                if std > 0:
                    eeg_data[ch] = (eeg_data[ch] - mean) / std
        
        except Exception as e:
            logger.error(f"åŠ è½½EEGæ–‡ä»¶å¤±è´¥: {item['eeg_path']}, é”™è¯¯: {e}")
            eeg_data = np.zeros((66, 4000), dtype=np.float32)
        
        text = item['sentence']
        
        return {
            'eeg': torch.from_numpy(eeg_data),
            'text': text,
            'id': item['id']
        }

def collate_fn_full_train(batch):
    """æ‰¹å¤„ç†å‡½æ•° - å…¨è®­ç»ƒé›†ç‰ˆæœ¬"""
    eeg_list = [item['eeg'] for item in batch]
    text_list = [item['text'] for item in batch]
    ids = [item['id'] for item in batch]
    
    return {
        'eeg': eeg_list,
        'text': text_list,
        'ids': ids
    }

class EncoderPretrainer:
    """EEGç¼–ç å™¨é¢„è®­ç»ƒå™¨"""
    
    def __init__(self, tokenizer, device='cuda' if torch.cuda.is_available() else 'cpu'):
        self.tokenizer = tokenizer
        self.device = device
        
        # è®­ç»ƒå†å²è®°å½•
        self.training_losses = []
        self.similarity_scores = []
        self.epoch_times = []
    
    def train_encoder_pretraining(self, train_loader, model, 
                                 epochs=100, learning_rate=1e-4, 
                                 save_path="encoder_pretrained.pth",
                                 eval_every_n_epochs=5):
        """é¢„è®­ç»ƒEEGç¼–ç å™¨"""
        
        print("\n" + "="*80)
        print("ğŸš€ EEGç¼–ç å™¨é¢„è®­ç»ƒ")
        print("ğŸ¯ ç›®æ ‡: é€šè¿‡å¯¹æ¯”å­¦ä¹ é¢„è®­ç»ƒEEGç¼–ç å™¨")
        print("ğŸ“š æ•°æ®ç­–ç•¥: å…¨éƒ¨æ•°æ®ç”¨äºè®­ç»ƒ")
        print("ğŸ”§ è®­ç»ƒç»„ä»¶: EEGç¼–ç å™¨ + æ–‡æœ¬ç¼–ç å™¨ï¼ˆå†»ç»“ï¼‰")
        print("="*80)
        
        # é…ç½®é˜¶æ®µ1è®­ç»ƒå‚æ•°
        model.configure_for_stage1()
        
        # åˆ†æå¯è®­ç»ƒå‚æ•°
        trainable_params = [p for p in model.parameters() if p.requires_grad]
        total_trainable = sum(p.numel() for p in trainable_params)
        total_params = sum(p.numel() for p in model.parameters())
        
        print(f"\nğŸ“Š æ¨¡å‹å‚æ•°ç»Ÿè®¡:")
        print(f"  æ€»å‚æ•°: {total_params:,}")
        print(f"  å¯è®­ç»ƒå‚æ•°: {total_trainable:,}")
        print(f"  å†»ç»“å‚æ•°: {total_params - total_trainable:,}")
        print(f"  è®­ç»ƒæ¯”ä¾‹: {total_trainable/total_params*100:.1f}%")
        
        # è¯¦ç»†å‚æ•°åˆ†æ
        eeg_patch_params = sum(p.numel() for p in model.patch_extractor.parameters())
        eeg_transformer_params = sum(p.numel() for p in model.eeg_encoder.parameters())
        eeg_proj_params = sum(p.numel() for p in model.eeg_projection.parameters())
        text_proj_params = sum(p.numel() for p in model.text_projection.parameters())
        
        print(f"\nğŸ” ç»„ä»¶å‚æ•°è¯¦æƒ…:")
        print(f"  ğŸ§  CNNç‰¹å¾æå–å™¨: {eeg_patch_params:,}")
        print(f"  ğŸ”„ EEG Transformer: {eeg_transformer_params:,}")
        print(f"  ğŸ“Š EEGæŠ•å½±å±‚: {eeg_proj_params:,}")
        print(f"  ğŸ“ æ–‡æœ¬æŠ•å½±å±‚: {text_proj_params:,}")
        
        # ä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡è°ƒåº¦å™¨
        optimizer = torch.optim.AdamW(trainable_params, lr=learning_rate, weight_decay=0.01)
        
        scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(
            optimizer, T_0=20, T_mult=2, eta_min=1e-7
        )
        
        # è®­ç»ƒå¾ªç¯
        best_loss = float('inf')
        patience = 25
        patience_counter = 0
        
        for epoch in range(epochs):
            epoch_start_time = datetime.now()
            print(f"\n--- Epoch {epoch+1}/{epochs} ---")
            
            # è®­ç»ƒ
            train_loss = self._train_epoch(model, train_loader, optimizer, epoch+1)
            
            scheduler.step()
            current_lr = optimizer.param_groups[0]['lr']
            
            epoch_end_time = datetime.now()
            epoch_duration = (epoch_end_time - epoch_start_time).total_seconds()
            
            print(f"ğŸ“ˆ è®­ç»ƒæŸå¤±: {train_loss:.4f}")
            print(f"âš¡ å­¦ä¹ ç‡: {current_lr:.2e}")
            print(f"â° è®­ç»ƒæ—¶é—´: {epoch_duration:.1f}ç§’")
            
            # è®°å½•å†å²
            self.training_losses.append(train_loss)
            self.epoch_times.append(epoch_duration)
            
            # å®šæœŸè¯„ä¼°
            if (epoch + 1) % eval_every_n_epochs == 0:
                print("\nğŸ§ª å¯¹æ¯”å­¦ä¹ æ•ˆæœè¯„ä¼°:")
                avg_similarity = self._evaluate_similarity(model, train_loader, num_samples=20)
                self.similarity_scores.append(avg_similarity)
                print(f"   åŒå¥EEG-æ–‡æœ¬ç›¸ä¼¼åº¦: {avg_similarity:.4f}")
            
            # æ—©åœå’Œä¿å­˜
            if train_loss < best_loss:
                best_loss = train_loss
                patience_counter = 0
                
                # ä¿å­˜æœ€ä½³æ¨¡å‹
                save_dict = {
                    'model_state_dict': model.state_dict(),
                    'epoch': epoch,
                    'train_loss': train_loss,
                    'best_loss': best_loss,
                    'config': {
                        'stage': 1,
                        'training_mode': 'contrastive_pretraining',
                        'patch_dim': model.patch_dim,
                        'feature_dim': model.feature_dim,
                        'target_channels': model.target_channels,
                        'target_length': model.target_length,
                        'temperature': model.temperature
                    },
                    'training_history': {
                        'losses': self.training_losses,
                        'similarities': self.similarity_scores,
                        'epoch_times': self.epoch_times
                    }
                }
                
                # ç¡®ä¿save_pathæ˜¯Pathå¯¹è±¡
                if isinstance(save_path, str):
                    save_path = Path(save_path)
                
                torch.save(save_dict, save_path)
                print(f"ğŸ’¾ ä¿å­˜æœ€ä½³æ¨¡å‹: {save_path}")
                
            else:
                patience_counter += 1
                print(f"â³ æ—©åœè®¡æ•°: {patience_counter}/{patience}")
                if patience_counter >= patience:
                    print(f"â¹ï¸ æ—©åœè§¦å‘ - è¿ç»­{patience}ä¸ªepochæ— æ”¹å–„")
                    break
        
        print(f"\nâœ… é¢„è®­ç»ƒå®Œæˆ! æœ€ä½³æŸå¤±: {best_loss:.4f}")
        print(f"ğŸ“Š æ€»è®­ç»ƒè½®æ•°: {len(self.training_losses)}")
        print(f"â° å¹³å‡æ¯è½®æ—¶é—´: {np.mean(self.epoch_times):.1f}ç§’")
        
        return model, best_loss
    
    def _train_epoch(self, model, train_loader, optimizer, epoch):
        """è®­ç»ƒä¸€ä¸ªepoch"""
        model.train()
        total_loss = 0.0
        num_batches = 0
        
        progress_bar = tqdm(train_loader, desc=f"ğŸ”¥ è®­ç»ƒ Epoch {epoch}")
        
        for batch in progress_bar:
            optimizer.zero_grad()
            
            try:
                # ğŸ”§ å°†EEGæ•°æ®ç§»åŠ¨åˆ°GPU
                if self.device.type == 'cuda':
                    batch['eeg'] = [eeg.to(self.device) for eeg in batch['eeg']]
                
                # å¯¹æ¯”å­¦ä¹ å‰å‘ä¼ æ’­
                outputs = model(
                    eeg_data=batch['eeg'],
                    text_data=batch['text'],
                    tokenizer=self.tokenizer
                )
                
                loss = outputs['loss']
                
                loss.backward()
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
                optimizer.step()
                
                total_loss += loss.item()
                num_batches += 1
                
                progress_bar.set_postfix({
                    'loss': f'{loss.item():.4f}',
                    'avg_loss': f'{total_loss/num_batches:.4f}'
                })
                
            except Exception as e:
                logger.error(f"è®­ç»ƒæ‰¹æ¬¡å‡ºé”™: {e}")
                continue
        
        return total_loss / num_batches if num_batches > 0 else float('inf')
    
    def _evaluate_similarity(self, model, train_loader, num_samples=20):
        """è¯„ä¼°åŒå¥å­EEG-æ–‡æœ¬ç›¸ä¼¼åº¦"""
        model.eval()
        similarities = []
        samples_processed = 0
        
        with torch.no_grad():
            for batch in train_loader:
                if samples_processed >= num_samples:
                    break
                
                try:
                    # ğŸ”§ å°†EEGæ•°æ®ç§»åŠ¨åˆ°GPU
                    if self.device.type == 'cuda':
                        batch['eeg'] = [eeg.to(self.device) for eeg in batch['eeg']]
                    
                    # å‰å‘ä¼ æ’­è·å–ç‰¹å¾
                    outputs = model(
                        eeg_data=batch['eeg'],
                        text_data=batch['text'],
                        tokenizer=self.tokenizer
                    )
                    
                    # è·å–EEGå’Œæ–‡æœ¬ç‰¹å¾
                    encoded_eeg = outputs['encoded_eeg']
                    text_features = outputs['text_features']
                    patch_lengths = outputs['patch_lengths']
                    
                    # è®¡ç®—EEGç‰¹å¾ï¼ˆæ± åŒ–ï¼‰
                    eeg_features = []
                    for i, length in enumerate(patch_lengths):
                        if length > 0:
                            valid_features = encoded_eeg[i, :length, :].mean(dim=0)
                        else:
                            valid_features = encoded_eeg[i, 0, :]
                        eeg_features.append(valid_features)
                    
                    eeg_features = torch.stack(eeg_features)
                    eeg_features = model.eeg_projection(eeg_features)
                    
                    # L2æ ‡å‡†åŒ–
                    eeg_features = nn.functional.normalize(eeg_features, p=2, dim=1)
                    text_features = nn.functional.normalize(text_features, p=2, dim=1)
                    
                    # è®¡ç®—å¯¹è§’çº¿ç›¸ä¼¼åº¦ï¼ˆåŒå¥å­çš„EEG-æ–‡æœ¬ç›¸ä¼¼åº¦ï¼‰
                    batch_similarities = torch.sum(eeg_features * text_features, dim=1)
                    similarities.extend(batch_similarities.cpu().numpy())
                    
                    samples_processed += len(batch['eeg'])
                    
                except Exception as e:
                    logger.error(f"ç›¸ä¼¼åº¦è¯„ä¼°å‡ºé”™: {e}")
                    continue
        
        return np.mean(similarities) if similarities else 0.0
    
    def analyze_same_sentence_similarity(self, model, train_loader, save_dir):
        """åˆ†æåŒå¥å­ä¸åŒEEGçš„ç›¸ä¼¼åº¦"""
        logger.info("ğŸ” åˆ†æåŒå¥å­ä¸åŒEEGæ ·æœ¬çš„ç›¸ä¼¼åº¦...")
        
        model.eval()
        sentence_groups = {}
        
        # æ”¶é›†æ‰€æœ‰å¥å­çš„EEGç‰¹å¾
        with torch.no_grad():
            for batch in tqdm(train_loader, desc="æ”¶é›†ç‰¹å¾"):
                try:
                    # ğŸ”§ å°†EEGæ•°æ®ç§»åŠ¨åˆ°GPU
                    if self.device.type == 'cuda':
                        batch['eeg'] = [eeg.to(self.device) for eeg in batch['eeg']]
                    
                    outputs = model(
                        eeg_data=batch['eeg'],
                        text_data=batch['text'],
                        tokenizer=self.tokenizer
                    )
                    
                    encoded_eeg = outputs['encoded_eeg']
                    patch_lengths = outputs['patch_lengths']
                    
                    # è®¡ç®—EEGç‰¹å¾
                    for i, (text, eeg_id) in enumerate(zip(batch['text'], batch['ids'])):
                        length = patch_lengths[i]
                        if length > 0:
                            eeg_feature = encoded_eeg[i, :length, :].mean(dim=0)
                        else:
                            eeg_feature = encoded_eeg[i, 0, :]
                        
                        eeg_feature = model.eeg_projection(eeg_feature.unsqueeze(0)).squeeze(0)
                        eeg_feature = nn.functional.normalize(eeg_feature, p=2, dim=0)
                        
                        if text not in sentence_groups:
                            sentence_groups[text] = []
                        sentence_groups[text].append({
                            'id': eeg_id,
                            'feature': eeg_feature.cpu().numpy()
                        })
                
                except Exception as e:
                    logger.error(f"ç‰¹å¾æ”¶é›†å‡ºé”™: {e}")
                    continue
        
        # åˆ†æåŒå¥å­ç›¸ä¼¼åº¦
        same_sentence_analysis = {}
        all_similarities = []
        
        for sentence, eeg_list in sentence_groups.items():
            if len(eeg_list) > 1:  # åªåˆ†ææœ‰å¤šä¸ªEEGçš„å¥å­
                similarities = []
                features = [item['feature'] for item in eeg_list]
                
                # è®¡ç®—ä¸¤ä¸¤ç›¸ä¼¼åº¦
                for i in range(len(features)):
                    for j in range(i+1, len(features)):
                        sim = np.dot(features[i], features[j])
                        similarities.append(sim)
                
                if similarities:
                    same_sentence_analysis[sentence] = {
                        'num_eegs': len(eeg_list),
                        'avg_similarity': float(np.mean(similarities)),
                        'std_similarity': float(np.std(similarities)),
                        'min_similarity': float(np.min(similarities)),
                        'max_similarity': float(np.max(similarities)),
                        'all_similarities': [float(s) for s in similarities],
                        'eeg_ids': [item['id'] for item in eeg_list]
                    }
                    all_similarities.extend(similarities)
        
        # æ•´ä½“ç»Ÿè®¡
        if all_similarities:
            overall_stats = {
                'num_sentences_with_multiple_eegs': len(same_sentence_analysis),
                'total_sentence_pairs': len(all_similarities),
                'avg_same_sentence_similarity': float(np.mean(all_similarities)),
                'std_same_sentence_similarity': float(np.std(all_similarities)),
                'min_same_sentence_similarity': float(np.min(all_similarities)),
                'max_same_sentence_similarity': float(np.max(all_similarities))
            }
        else:
            overall_stats = {
                'num_sentences_with_multiple_eegs': 0,
                'total_sentence_pairs': 0,
                'avg_same_sentence_similarity': 0.0,
                'std_same_sentence_similarity': 0.0,
                'min_same_sentence_similarity': 0.0,
                'max_same_sentence_similarity': 0.0
            }
        
        # ä¿å­˜åˆ†æç»“æœ
        analysis_results = {
            'per_sentence_analysis': same_sentence_analysis,
            'overall_statistics': overall_stats,
            'analysis_timestamp': datetime.now().isoformat()
        }
        
        analysis_path = save_dir / 'same_sentence_similarity_analysis.json'
        with open(analysis_path, 'w', encoding='utf-8') as f:
            json.dump(analysis_results, f, ensure_ascii=False, indent=2)
        
        # æ‰“å°ç»“æœ
        logger.info(f"\nğŸ“Š åŒå¥å­ç›¸ä¼¼åº¦åˆ†æç»“æœ:")
        logger.info(f"  æœ‰å¤šä¸ªEEGçš„å¥å­æ•°é‡: {overall_stats['num_sentences_with_multiple_eegs']}")
        logger.info(f"  æ€»çš„å¥å­å¯¹æ•°é‡: {overall_stats['total_sentence_pairs']}")
        if overall_stats['total_sentence_pairs'] > 0:
            logger.info(f"  å¹³å‡ç›¸ä¼¼åº¦: {overall_stats['avg_same_sentence_similarity']:.4f}")
            logger.info(f"  æ ‡å‡†å·®: {overall_stats['std_same_sentence_similarity']:.4f}")
            logger.info(f"  æœ€é«˜ç›¸ä¼¼åº¦: {overall_stats['max_same_sentence_similarity']:.4f}")
            logger.info(f"  æœ€ä½ç›¸ä¼¼åº¦: {overall_stats['min_same_sentence_similarity']:.4f}")
        
        logger.info(f"ğŸ“ è¯¦ç»†åˆ†æä¿å­˜åˆ°: {analysis_path}")
        
        return analysis_results

def main():
    """ä¸»é¢„è®­ç»ƒå‡½æ•°"""
    config = {
        'csv_file': 'additional_data/subject1_gxy_0328/gxy_0328.csv',
        'eeg_dir': 'additional_data/subject1_gxy_0328/',
        'batch_size': 32,  # ç¨å¾®å¤§ä¸€ç‚¹çš„batch sizeç”¨äºé¢„è®­ç»ƒ
        
        # é¢„è®­ç»ƒå‚æ•°
        'epochs': 150,
        'learning_rate': 1e-4,
        'eval_every_n_epochs': 5,
        
        # æ¨¡å‹å‚æ•°
        'patch_dim': 256,
        'feature_dim': 768,
        'target_channels': 66,
        'target_length': 4000,
        'temperature': 0.07,
        
        'save_dir': f'encoder_pretrain_full_{datetime.now().strftime("%Y%m%d_%H%M%S")}'
    }
    
    logger.info("=== EEGç¼–ç å™¨é¢„è®­ç»ƒï¼ˆå…¨æ•°æ®é›†ï¼‰ ===")
    logger.info("ğŸ¯ è®­ç»ƒç­–ç•¥:")
    logger.info("  ğŸ“š å¯¹æ¯”å­¦ä¹ é¢„è®­ç»ƒEEGç¼–ç å™¨")
    logger.info("  ğŸ”’ å†»ç»“æ–‡æœ¬ç¼–ç å™¨ï¼ˆBARTç¼–ç å™¨ï¼‰")
    logger.info("  ğŸ“Š å…¨éƒ¨æ•°æ®ç”¨äºè®­ç»ƒ")
    logger.info("  ğŸ¯ ç›®æ ‡ï¼šå­¦ä¹ EEG-æ–‡æœ¬å¯¹åº”å…³ç³»")
    
    # æ£€æŸ¥æ•°æ®æ–‡ä»¶
    if not os.path.exists(config['csv_file']):
        logger.error(f"âŒ CSVæ–‡ä»¶ä¸å­˜åœ¨: {config['csv_file']}")
        return
    
    if not os.path.exists(config['eeg_dir']):
        logger.error(f"âŒ EEGç›®å½•ä¸å­˜åœ¨: {config['eeg_dir']}")
        return
    
    # åˆ›å»ºä¿å­˜ç›®å½•
    save_dir = Path(config['save_dir'])
    save_dir.mkdir(parents=True, exist_ok=True)
    
    # ä¿å­˜é…ç½®
    config_path = save_dir / 'config.json'
    with open(config_path, 'w', encoding='utf-8') as f:
        json.dump(config, f, ensure_ascii=False, indent=2)
    logger.info(f"ğŸ“ é…ç½®ä¿å­˜åˆ°: {config_path}")
    
    # åˆå§‹åŒ–tokenizer
    logger.info("ğŸ”§ åˆå§‹åŒ–tokenizer...")
    try:
        from transformers import AutoTokenizer
        tokenizer = AutoTokenizer.from_pretrained('fnlp/bart-base-chinese')
        logger.info(f"âœ… æˆåŠŸåŠ è½½BART tokenizer")
    except Exception as e:
        logger.error(f"âŒ tokenizeråŠ è½½å¤±è´¥: {e}")
        return
    
    # åˆ›å»ºæ•°æ®é›†
    logger.info("ğŸ“š åˆ›å»ºå…¨è®­ç»ƒæ•°æ®é›†...")
    full_dataset = EEGTextDatasetFullTrain(
        csv_file=config['csv_file'],
        eeg_dir=config['eeg_dir'],
        tokenizer=tokenizer
    )
    
    if len(full_dataset) == 0:
        logger.error("âŒ æ•°æ®é›†ä¸ºç©ºï¼Œæ— æ³•è®­ç»ƒ!")
        return
    
    logger.info(f"âœ… æ•°æ®é›†åˆ›å»ºå®Œæˆï¼Œæ€»æ ·æœ¬æ•°: {len(full_dataset)}")
    
    # åˆ†ææ•°æ®é›†
    logger.info("ğŸ” åˆ†ææ•°æ®é›†...")
    sentences = [item['sentence'] for item in full_dataset.valid_data]
    unique_sentences = set(sentences)
    logger.info(f"  æ€»å¥å­æ•°: {len(sentences)}")
    logger.info(f"  å”¯ä¸€å¥å­æ•°: {len(unique_sentences)}")
    logger.info(f"  å¹³å‡æ¯å¥EEGæ•°: {len(sentences)/len(unique_sentences):.2f}")
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_loader = DataLoader(
        full_dataset, 
        batch_size=config['batch_size'], 
        shuffle=True, 
        collate_fn=collate_fn_full_train, 
        num_workers=2,
        drop_last=True  # ç¡®ä¿æ‰¹æ¬¡å¤§å°ä¸€è‡´
    )
    
    logger.info(f"ğŸ“Š æ•°æ®åŠ è½½å™¨åˆ›å»ºå®Œæˆ:")
    logger.info(f"  æ‰¹æ¬¡å¤§å°: {config['batch_size']}")
    logger.info(f"  æ€»æ‰¹æ¬¡æ•°: {len(train_loader)}")
    
    # åˆ›å»ºæ¨¡å‹
    logger.info("ğŸ—ï¸ åˆ›å»ºå¯¹æ¯”å­¦ä¹ æ¨¡å‹...")
    model = ContrastiveEEGTextModel(
        bart_model_name="fnlp/bart-base-chinese",
        patch_dim=config['patch_dim'],
        feature_dim=config['feature_dim'],
        target_channels=config['target_channels'],
        target_length=config['target_length'],
        temperature=config['temperature']
    )
    
    # ç§»åŠ¨åˆ°è®¾å¤‡
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    logger.info(f"ğŸ–¥ï¸ ä½¿ç”¨è®¾å¤‡: {device}")
    
    # æ£€æŸ¥GPUå†…å­˜
    if device.type == 'cuda':
        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9
        logger.info(f"ğŸ® GPUå†…å­˜: {gpu_memory:.1f} GB")
    
    model = model.to(device)
    
    # åˆ›å»ºé¢„è®­ç»ƒå™¨
    pretrainer = EncoderPretrainer(tokenizer, device)
    
    # å¼€å§‹é¢„è®­ç»ƒ
    model_path = save_dir / 'stage1_contrastive_pretrained.pth'
    final_model, best_loss = pretrainer.train_encoder_pretraining(
        train_loader=train_loader,
        model=model,
        epochs=config['epochs'],
        learning_rate=config['learning_rate'],
        save_path=model_path,
        eval_every_n_epochs=config['eval_every_n_epochs']
    )
    
    # åˆ†æåŒå¥å­ç›¸ä¼¼åº¦
    logger.info("\n" + "="*80)
    logger.info("ğŸ” åŒå¥å­EEGç›¸ä¼¼åº¦åˆ†æ")
    logger.info("="*80)
    
    # åŠ è½½æœ€ä½³æ¨¡å‹è¿›è¡Œåˆ†æ
    best_checkpoint = torch.load(model_path, map_location=device)
    final_model.load_state_dict(best_checkpoint['model_state_dict'])
    
    similarity_analysis = pretrainer.analyze_same_sentence_similarity(
        final_model, train_loader, save_dir
    )
    
    # ç»˜åˆ¶è®­ç»ƒå†å²
    if pretrainer.training_losses:
        plt.figure(figsize=(15, 10))
        
        # è®­ç»ƒæŸå¤±
        plt.subplot(2, 3, 1)
        epochs = range(1, len(pretrainer.training_losses) + 1)
        plt.plot(epochs, pretrainer.training_losses, 'b-', linewidth=2)
        plt.xlabel('Epoch')
        plt.ylabel('å¯¹æ¯”æŸå¤±')
        plt.title('è®­ç»ƒæŸå¤±æ›²çº¿')
        plt.grid(True, alpha=0.3)
        
        # ç›¸ä¼¼åº¦å˜åŒ–
        plt.subplot(2, 3, 2)
        if pretrainer.similarity_scores:
            sim_epochs = range(config['eval_every_n_epochs'], 
                             len(pretrainer.similarity_scores) * config['eval_every_n_epochs'] + 1, 
                             config['eval_every_n_epochs'])
            plt.plot(sim_epochs, pretrainer.similarity_scores, 'g-', linewidth=2, marker='o')
            plt.xlabel('Epoch')
            plt.ylabel('åŒå¥EEG-æ–‡æœ¬ç›¸ä¼¼åº¦')
            plt.title('ç›¸ä¼¼åº¦å˜åŒ–')
            plt.grid(True, alpha=0.3)
        
        # è®­ç»ƒæ—¶é—´
        plt.subplot(2, 3, 3)
        plt.plot(epochs, pretrainer.epoch_times, 'r-', linewidth=2)
        plt.xlabel('Epoch')
        plt.ylabel('æ—¶é—´ (ç§’)')
        plt.title('æ¯è½®è®­ç»ƒæ—¶é—´')
        plt.grid(True, alpha=0.3)
        
        # æœ€ç»ˆç›¸ä¼¼åº¦åˆ†å¸ƒ
        plt.subplot(2, 3, 4)
        if 'overall_statistics' in similarity_analysis:
            stats = similarity_analysis['overall_statistics']
            metrics = ['å¹³å‡ç›¸ä¼¼åº¦', 'æœ€é«˜ç›¸ä¼¼åº¦', 'æœ€ä½ç›¸ä¼¼åº¦']
            values = [
                stats['avg_same_sentence_similarity'],
                stats['max_same_sentence_similarity'],
                stats['min_same_sentence_similarity']
            ]
            plt.bar(metrics, values, color=['skyblue', 'lightgreen', 'lightcoral'])
            plt.ylabel('ç›¸ä¼¼åº¦')
            plt.title('åŒå¥å­EEGç›¸ä¼¼åº¦åˆ†æ')
            plt.xticks(rotation=45)
            plt.grid(True, alpha=0.3)
        
        # è®­ç»ƒæ‘˜è¦
        plt.subplot(2, 3, 5)
        summary_text = [
            f"è®­ç»ƒè½®æ•°: {len(pretrainer.training_losses)}",
            f"æœ€ä½³æŸå¤±: {best_loss:.4f}",
            f"æ•°æ®é›†å¤§å°: {len(full_dataset)}",
            f"å”¯ä¸€å¥å­: {len(unique_sentences)}",
            f"æ‰¹æ¬¡å¤§å°: {config['batch_size']}",
            f"å­¦ä¹ ç‡: {config['learning_rate']:.0e}"
        ]
        
        plt.text(0.1, 0.8, "ğŸ¯ é¢„è®­ç»ƒæ€»ç»“", fontsize=14, fontweight='bold')
        for i, text in enumerate(summary_text):
            plt.text(0.1, 0.7 - i*0.1, text, fontsize=10)
        plt.xlim(0, 1)
        plt.ylim(0, 1)
        plt.axis('off')
        
        # æŸå¤±æ”¹å–„è¶‹åŠ¿
        plt.subplot(2, 3, 6)
        if len(pretrainer.training_losses) > 10:
            # è®¡ç®—æ»‘åŠ¨å¹³å‡
            window_size = min(10, len(pretrainer.training_losses) // 4)
            smoothed_losses = []
            for i in range(window_size, len(pretrainer.training_losses)):
                smoothed_losses.append(
                    np.mean(pretrainer.training_losses[i-window_size:i])
                )
            
            smooth_epochs = range(window_size+1, len(pretrainer.training_losses) + 1)
            plt.plot(epochs, pretrainer.training_losses, 'b-', alpha=0.3, label='åŸå§‹æŸå¤±')
            plt.plot(smooth_epochs, smoothed_losses, 'r-', linewidth=2, label=f'æ»‘åŠ¨å¹³å‡({window_size})')
            plt.xlabel('Epoch')
            plt.ylabel('æŸå¤±')
            plt.title('æŸå¤±å¹³æ»‘è¶‹åŠ¿')
            plt.legend()
            plt.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plot_path = save_dir / 'pretraining_results.png'
        plt.savefig(plot_path, dpi=300, bbox_inches='tight')
        plt.show()
        
        logger.info(f"ğŸ“Š è®­ç»ƒå†å²å›¾ä¿å­˜åˆ°: {plot_path}")
    
    # ä¿å­˜å®Œæ•´ç»“æœ
    results = {
        'best_loss': best_loss,
        'training_epochs': len(pretrainer.training_losses),
        'training_history': {
            'losses': pretrainer.training_losses,
            'similarities': pretrainer.similarity_scores,
            'epoch_times': pretrainer.epoch_times
        },
        'dataset_info': {
            'total_samples': len(full_dataset),
            'unique_sentences': len(unique_sentences),
            'avg_eegs_per_sentence': len(sentences) / len(unique_sentences)
        },
        'similarity_analysis': similarity_analysis,
        'config': config,
        'training_strategy': 'contrastive_pretraining_full_dataset',
        'timestamp': datetime.now().isoformat()
    }
    
    results_path = save_dir / 'pretraining_results.json'
    with open(results_path, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    
    # å®Œæˆä¿¡æ¯
    logger.info(f"\nğŸ‰ EEGç¼–ç å™¨é¢„è®­ç»ƒå®Œæˆ!")
    logger.info(f"ğŸ“Š æœ€ä½³å¯¹æ¯”æŸå¤±: {best_loss:.4f}")
    logger.info(f"ğŸ“Š è®­ç»ƒè½®æ•°: {len(pretrainer.training_losses)}")
    logger.info(f"ğŸ“Š æ•°æ®é›†å¤§å°: {len(full_dataset)}")
    
    if pretrainer.similarity_scores:
        logger.info(f"ğŸ“Š æœ€ç»ˆåŒå¥ç›¸ä¼¼åº¦: {pretrainer.similarity_scores[-1]:.4f}")
    
    if 'overall_statistics' in similarity_analysis:
        stats = similarity_analysis['overall_statistics']
        logger.info(f"ğŸ“Š åŒå¥EEGå¹³å‡ç›¸ä¼¼åº¦: {stats['avg_same_sentence_similarity']:.4f}")
    
    logger.info(f"ğŸ“‚ ç»“æœä¿å­˜åœ¨: {save_dir}")
    logger.info(f"ğŸ’¾ é¢„è®­ç»ƒæ¨¡å‹: {model_path}")
    logger.info(f"ğŸ“Š ç›¸ä¼¼åº¦åˆ†æ: {save_dir / 'same_sentence_similarity_analysis.json'}")
    logger.info(f"ğŸ“ˆ è®­ç»ƒå†å²: {results_path}")

if __name__ == "__main__":
    main()

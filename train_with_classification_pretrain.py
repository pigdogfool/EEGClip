import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader, Subset
import numpy as np
import pandas as pd
import os
from tqdm import tqdm
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
import logging
import json
from datetime import datetime
from pathlib import Path

# âœ… æ·»åŠ BLEUåˆ†æ•°è®¡ç®—ç›¸å…³å¯¼å…¥
try:
    from nltk.translate.bleu_score import sentence_bleu, corpus_bleu, SmoothingFunction
    from nltk.tokenize import word_tokenize
    import nltk
    
    # ä¸‹è½½å¿…è¦çš„NLTKæ•°æ®
    try:
        nltk.data.find('tokenizers/punkt')
    except LookupError:
        nltk.download('punkt')
    
    NLTK_AVAILABLE = True
except ImportError:
    print("âš ï¸ NLTKæœªå®‰è£…ï¼Œå°†ä½¿ç”¨ç®€å•çš„å­—ç¬¦çº§BLEUè®¡ç®—")
    NLTK_AVAILABLE = False

from eeg_to_text_model_contrastive import ContrastiveEEGTextModel

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# âœ… BLEUåˆ†æ•°è®¡ç®—å™¨
class BLEUCalculator:
    """BLEUåˆ†æ•°è®¡ç®—å™¨"""
    
    def __init__(self, use_smoothing=True):
        self.use_smoothing = use_smoothing
        if NLTK_AVAILABLE and use_smoothing:
            self.smoothing_function = SmoothingFunction().method1
        else:
            self.smoothing_function = None
    
    def _tokenize_chinese(self, text):
        """ä¸­æ–‡åˆ†è¯ - å­—ç¬¦çº§åˆ«æˆ–è¯çº§åˆ«"""
        if NLTK_AVAILABLE:
            try:
                # å°è¯•ä½¿ç”¨NLTKåˆ†è¯
                tokens = word_tokenize(text)
                return tokens
            except:
                # å¦‚æœå¤±è´¥ï¼Œä½¿ç”¨å­—ç¬¦çº§åˆ†è¯
                return list(text.replace(' ', ''))
        else:
            # ç®€å•çš„å­—ç¬¦çº§åˆ†è¯ï¼ˆé€‚åˆä¸­æ–‡ï¼‰
            return list(text.replace(' ', ''))
    
    def calculate_sentence_bleu(self, reference, candidate, weights=(0.25, 0.25, 0.25, 0.25)):
        """è®¡ç®—å•å¥BLEUåˆ†æ•°"""
        if not reference or not candidate:
            return 0.0
        
        # åˆ†è¯
        ref_tokens = self._tokenize_chinese(reference)
        cand_tokens = self._tokenize_chinese(candidate)
        
        if len(ref_tokens) == 0 or len(cand_tokens) == 0:
            return 0.0
        
        if NLTK_AVAILABLE:
            try:
                # ä½¿ç”¨NLTKè®¡ç®—BLEU
                bleu_score = sentence_bleu(
                    [ref_tokens], 
                    cand_tokens, 
                    weights=weights,
                    smoothing_function=self.smoothing_function
                )
                return bleu_score
            except:
                # å¦‚æœNLTKå¤±è´¥ï¼Œä½¿ç”¨ç®€å•æ–¹æ³•
                return self._simple_bleu(ref_tokens, cand_tokens)
        else:
            return self._simple_bleu(ref_tokens, cand_tokens)
    
    def _simple_bleu(self, ref_tokens, cand_tokens):
        """ç®€å•çš„BLEUè®¡ç®—ï¼ˆå½“NLTKä¸å¯ç”¨æ—¶ï¼‰"""
        # è®¡ç®—1-gramç²¾ç¡®åº¦
        ref_set = set(ref_tokens)
        cand_set = set(cand_tokens)
        
        if len(cand_set) == 0:
            return 0.0
        
        precision_1 = len(ref_set.intersection(cand_set)) / len(cand_set)
        
        # ç®€åŒ–çš„é•¿åº¦æƒ©ç½š
        bp = min(1.0, len(cand_tokens) / len(ref_tokens)) if len(ref_tokens) > 0 else 0.0
        
        return bp * precision_1
    
    def calculate_corpus_bleu(self, references, candidates, weights=(0.25, 0.25, 0.25, 0.25)):
        """è®¡ç®—è¯­æ–™åº“çº§BLEUåˆ†æ•°"""
        if len(references) != len(candidates):
            raise ValueError("å‚è€ƒæ–‡æœ¬å’Œå€™é€‰æ–‡æœ¬æ•°é‡ä¸åŒ¹é…")
        
        if not references or not candidates:
            return 0.0
        
        # åˆ†è¯æ‰€æœ‰æ–‡æœ¬
        ref_tokens_list = []
        cand_tokens_list = []
        
        for ref, cand in zip(references, candidates):
            if ref and cand:  # è·³è¿‡ç©ºæ–‡æœ¬
                ref_tokens = self._tokenize_chinese(ref)
                cand_tokens = self._tokenize_chinese(cand)
                
                if len(ref_tokens) > 0 and len(cand_tokens) > 0:
                    ref_tokens_list.append([ref_tokens])  # NLTKéœ€è¦åµŒå¥—åˆ—è¡¨
                    cand_tokens_list.append(cand_tokens)
        
        if not ref_tokens_list or not cand_tokens_list:
            return 0.0
        
        if NLTK_AVAILABLE:
            try:
                bleu_score = corpus_bleu(
                    ref_tokens_list, 
                    cand_tokens_list, 
                    weights=weights,
                    smoothing_function=self.smoothing_function
                )
                return bleu_score
            except:
                # å¦‚æœNLTKå¤±è´¥ï¼Œè®¡ç®—å¹³å‡å¥å­BLEU
                scores = []
                for ref_tokens, cand_tokens in zip(ref_tokens_list, cand_tokens_list):
                    score = self._simple_bleu(ref_tokens[0], cand_tokens)
                    scores.append(score)
                return np.mean(scores) if scores else 0.0
        else:
            # è®¡ç®—å¹³å‡å¥å­BLEU
            scores = []
            for ref_tokens, cand_tokens in zip(ref_tokens_list, cand_tokens_list):
                score = self._simple_bleu(ref_tokens[0], cand_tokens)
                scores.append(score)
            return np.mean(scores) if scores else 0.0

class EEGTextDataset(Dataset):
    """EEGå’Œæ–‡æœ¬é…å¯¹æ•°æ®é›†"""
    
    def __init__(self, csv_file, eeg_dir, tokenizer, max_text_length=32):
        self.data = pd.read_csv(csv_file)
        self.eeg_dir = eeg_dir
        self.tokenizer = tokenizer
        self.max_text_length = max_text_length
        
        # è¿‡æ»¤æ‰æ²¡æœ‰å¯¹åº”EEGæ–‡ä»¶çš„æ•°æ®
        self.valid_data = []
        not_found_ids = []
        
        for idx, row in self.data.iterrows():
            possible_patterns = [
                f"preprocessed_{row['id']}_raw_300_0328.npy",
                f"preprocessed_{row['id']}.npy",
                f"{row['id']}_raw_300_0328.npy",
                f"{row['id']}.npy",
            ]
            
            eeg_path = None
            for pattern in possible_patterns:
                test_path = os.path.join(eeg_dir, pattern)
                if os.path.exists(test_path):
                    eeg_path = test_path
                    break
            
            if eeg_path:
                self.valid_data.append({
                    'id': row['id'],
                    'sentence': row['sentence'],
                    'eeg_path': eeg_path
                })
            else:
                not_found_ids.append(row['id'])
        
        logger.info(f"æ‰¾åˆ° {len(self.valid_data)} ä¸ªæœ‰æ•ˆçš„EEG-æ–‡æœ¬é…å¯¹")
        if len(not_found_ids) > 0:
            logger.warning(f"æœªæ‰¾åˆ°EEGæ–‡ä»¶çš„IDæ•°é‡: {len(not_found_ids)}")
    
    def __len__(self):
        return len(self.valid_data)
    
    def __getitem__(self, idx):
        item = self.valid_data[idx]
        
        # åŠ è½½EEGæ•°æ®
        try:
            eeg_data = np.load(item['eeg_path'])
            eeg_data = eeg_data.astype(np.float32)
            
            # æ ‡å‡†åŒ–æ¯ä¸ªé€šé“
            for ch in range(eeg_data.shape[0]):
                mean = np.mean(eeg_data[ch])
                std = np.std(eeg_data[ch])
                if std > 0:
                    eeg_data[ch] = (eeg_data[ch] - mean) / std
        
        except Exception as e:
            logger.error(f"åŠ è½½EEGæ–‡ä»¶å¤±è´¥: {item['eeg_path']}, é”™è¯¯: {e}")
            eeg_data = np.zeros((66, 4000), dtype=np.float32)
        
        text = item['sentence']
        
        return {
            'eeg': torch.from_numpy(eeg_data),
            'text': text,
            'id': item['id']
        }

def collate_fn(batch):
    """æ‰¹å¤„ç†å‡½æ•°"""
    eeg_list = [item['eeg'] for item in batch]
    text_list = [item['text'] for item in batch]
    ids = [item['id'] for item in batch]
    
    return {
        'eeg': eeg_list,
        'text': text_list,
        'ids': ids
    }

class IntegratedEEGToTextModel(nn.Module):
    """æ•´åˆåˆ†ç±»é¢„è®­ç»ƒçš„EEG-to-Textæ¨¡å‹"""
    
    def __init__(self, pretrained_classification_model, bart_model_name="fnlp/bart-base-chinese"):
        super(IntegratedEEGToTextModel, self).__init__()
        
        # ä½¿ç”¨é¢„è®­ç»ƒçš„åˆ†ç±»æ¨¡å‹ä½œä¸ºEEGç¼–ç å™¨
        self.eeg_encoder = pretrained_classification_model
        
        # BARTæ¨¡å‹ç”¨äºæ–‡æœ¬ç”Ÿæˆ
        from transformers import BartForConditionalGeneration
        self.bart = BartForConditionalGeneration.from_pretrained(bart_model_name)
        
        # âœ… æŠ•å½±å±‚ï¼šä»patch_dimæŠ•å½±åˆ°BARTéšè—ç»´åº¦
        eeg_patch_dim = self.eeg_encoder.patch_dim  # 256
        bart_hidden_dim = self.bart.config.d_model   # 768
        
        self.eeg_to_bart_projection = nn.Linear(eeg_patch_dim, bart_hidden_dim)

    def configure_for_stage2(self, freeze_eeg_encoder=True):
        """é…ç½®é˜¶æ®µ2è®­ç»ƒå‚æ•°"""
        if freeze_eeg_encoder:
            print("ğŸ”’ å†»ç»“EEGç¼–ç å™¨ï¼ˆä½¿ç”¨é¢„è®­ç»ƒç‰¹å¾ï¼‰")
            # âœ… ç›´æ¥å†»ç»“æ•´ä¸ªEEGç¼–ç å™¨
            for param in self.eeg_encoder.parameters():
                param.requires_grad = False
        else:
            print("ğŸ”“ è§£å†»EEGç¼–ç å™¨ï¼ˆè”åˆå¾®è°ƒï¼‰")
            for param in self.eeg_encoder.parameters():
                param.requires_grad = True
        
        # è§£å†»BARTè§£ç å™¨
        for param in self.bart.model.decoder.parameters():
            param.requires_grad = True
        for param in self.bart.lm_head.parameters():
            param.requires_grad = True
        
        # è§£å†»æŠ•å½±å±‚
        for param in self.eeg_to_bart_projection.parameters():
            param.requires_grad = True

    def forward(self, eeg_data, text_data=None, tokenizer=None):
        """å‰å‘ä¼ æ’­"""
        # âœ… ç›´æ¥ä½¿ç”¨é¢„è®­ç»ƒç¼–ç å™¨çš„encode_eegæ–¹æ³•
        encoded_eeg, eeg_mask, patch_lengths = self.eeg_encoder.encode_eeg(eeg_data)
        
        # âœ… æŠ•å½±åˆ°BARTç©ºé—´ï¼š(batch_size, seq_len, patch_dim) â†’ (batch_size, seq_len, bart_hidden_dim)
        bart_sequence = self.eeg_to_bart_projection(encoded_eeg)
        
        eeg_mask = eeg_mask.to(encoded_eeg.device)
        
        # è½¬æ¢maskæ ¼å¼ï¼šEEG mask (True=æ— æ•ˆ) â†’ BART attention mask (True=æœ‰æ•ˆ)
        encoder_attention_mask = ~eeg_mask
        
        if text_data is not None and tokenizer is not None:
            # è®­ç»ƒæ¨¡å¼ï¼šè®¡ç®—ç”ŸæˆæŸå¤±
            encodings = tokenizer(
                text_data,
                padding='max_length',
                max_length=32,
                truncation=True,
                return_tensors='pt'
            )
            
            input_ids = encodings['input_ids'].to(bart_sequence.device)
            attention_mask = encodings['attention_mask'].to(bart_sequence.device)
            
            # å‡†å¤‡è§£ç å™¨è¾“å…¥
            decoder_input_ids = input_ids[:, :-1]
            labels = input_ids[:, 1:].clone()
            labels[labels == tokenizer.pad_token_id] = -100
            
            # BARTå‰å‘ä¼ æ’­
            from transformers.modeling_outputs import BaseModelOutput
            encoder_outputs = BaseModelOutput(
                last_hidden_state=bart_sequence,
                hidden_states=None,
                attentions=None
            )
            
            outputs = self.bart(
                encoder_outputs=encoder_outputs,
                attention_mask=encoder_attention_mask,  # EEGçš„attention mask
                decoder_input_ids=decoder_input_ids,
                decoder_attention_mask=attention_mask[:, :-1],  # æ–‡æœ¬çš„attention mask
                labels=labels,
                return_dict=True
            )
            
            return outputs
        else:
            # æ¨ç†æ¨¡å¼
            from transformers.modeling_outputs import BaseModelOutput
            encoder_outputs = BaseModelOutput(
                last_hidden_state=bart_sequence,
                hidden_states=None,
                attentions=None
            )
            return encoder_outputs, encoder_attention_mask

    def generate_text(self, eeg_data, tokenizer, max_length=32, num_beams=4, **kwargs):
        """ç”Ÿæˆæ–‡æœ¬"""
        with torch.no_grad():
            # âœ… è·å–EEGç‰¹å¾å¹¶æŠ•å½±
            encoded_eeg, eeg_mask, patch_lengths = self.eeg_encoder.encode_eeg(eeg_data)
            bart_sequence = self.eeg_to_bart_projection(encoded_eeg)
            
            # è·å–attention mask
            eeg_mask = eeg_mask.to(encoded_eeg.device)
            encoder_attention_mask = ~eeg_mask
            
            # åˆ›å»ºencoder_outputs
            from transformers.modeling_outputs import BaseModelOutput
            encoder_outputs = BaseModelOutput(
                last_hidden_state=bart_sequence,
                hidden_states=None,
                attentions=None
            )
            
            generated_ids = self.bart.generate(
                encoder_outputs=encoder_outputs,
                attention_mask=encoder_attention_mask,
                max_length=max_length,
                num_beams=num_beams,
                early_stopping=True,
                pad_token_id=tokenizer.pad_token_id,
                eos_token_id=tokenizer.sep_token_id,
                **kwargs
            )
            
            # è§£ç ç”Ÿæˆçš„æ–‡æœ¬
            generated_texts = []
            for ids in generated_ids:
                text = tokenizer.decode(ids, skip_special_tokens=True)
                generated_texts.append(text)
            
            return generated_texts

# âœ… ä¿®æ”¹ä¸ºè®­ç»ƒé›†/æµ‹è¯•é›†äºŒåˆ†å‰²
def create_train_test_split(dataset, test_size=0.2, random_state=42):
    """åˆ›å»ºè®­ç»ƒ/æµ‹è¯•é›†åˆ†å‰²"""
    logger.info("ğŸ”„ åˆ›å»ºäºŒåˆ†å‰²æ•°æ®é›† (è®­ç»ƒ/æµ‹è¯•)")
    
    total_size = len(dataset)
    indices = list(range(total_size))
    
    # åˆ†å‰²ä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†
    train_indices, test_indices = train_test_split(
        indices, 
        test_size=test_size, 
        random_state=random_state,
        shuffle=True
    )
    
    logger.info(f"ğŸ“Š æ•°æ®é›†åˆ†å‰²ç»“æœ:")
    logger.info(f"  ğŸ“ è®­ç»ƒé›†: {len(train_indices)} æ ·æœ¬ ({len(train_indices)/total_size*100:.1f}%)")
    logger.info(f"  ğŸ§ª æµ‹è¯•é›†: {len(test_indices)} æ ·æœ¬ ({len(test_indices)/total_size*100:.1f}%)")
    
    return train_indices, test_indices

def save_data_splits(train_indices, test_indices, save_dir):
    """ä¿å­˜æ•°æ®åˆ†å‰²ç´¢å¼•"""
    splits_dir = save_dir / 'data_splits'
    splits_dir.mkdir(exist_ok=True)
    
    np.save(splits_dir / 'train_indices.npy', train_indices)
    np.save(splits_dir / 'test_indices.npy', test_indices)
    
    # ä¿å­˜åˆ†å‰²ä¿¡æ¯
    split_info = {
        'train_size': len(train_indices),
        'test_size': len(test_indices),
        'total_size': len(train_indices) + len(test_indices),
        'split_ratios': {
            'train': len(train_indices) / (len(train_indices) + len(test_indices)),
            'test': len(test_indices) / (len(train_indices) + len(test_indices))
        },
        'creation_time': datetime.now().isoformat()
    }
    
    with open(splits_dir / 'split_info.json', 'w', encoding='utf-8') as f:
        json.dump(split_info, f, ensure_ascii=False, indent=2)
    
    logger.info(f"ğŸ“ æ•°æ®åˆ†å‰²ç´¢å¼•ä¿å­˜åˆ°: {splits_dir}")

class IntegratedTrainer:
    """æ•´åˆçš„è®­ç»ƒå™¨"""
    
    def __init__(self, tokenizer, device='cuda' if torch.cuda.is_available() else 'cpu'):
        self.tokenizer = tokenizer
        self.device = device
        
        # è®­ç»ƒå†å²è®°å½•
        self.training_losses = []
        
        # âœ… æ·»åŠ BLEUè®¡ç®—å™¨
        self.bleu_calculator = BLEUCalculator(use_smoothing=True)
    
    def train_with_test_evaluation(self, train_loader, test_loader, pretrained_model, 
                                  epochs=60, learning_rate=2e-5, 
                                  save_path="final_model.pth", freeze_eeg=True):
        """è®­ç»ƒæ¨¡å‹å¹¶åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°"""
        print("\n" + "="*80)
        print("ğŸ”¥ EEG-to-Textæ¨¡å‹è®­ç»ƒ")
        print("ğŸ¯ ç›®æ ‡: ä½¿ç”¨é¢„è®­ç»ƒçš„EEGç¼–ç å™¨è®­ç»ƒæ–‡æœ¬ç”Ÿæˆ")
        print(f"ğŸ”’ EEGç¼–ç å™¨: {'å†»ç»“' if freeze_eeg else 'å¾®è°ƒ'}")
        print("ğŸ”“ BARTè§£ç å™¨: å¾®è°ƒ")
        print("="*80)
        
        # åˆ›å»ºæ•´åˆæ¨¡å‹
        integrated_model = IntegratedEEGToTextModel(
            pretrained_classification_model=pretrained_model,
            bart_model_name="fnlp/bart-base-chinese"
        ).to(self.device)
        
        # é…ç½®è®­ç»ƒå‚æ•°
        integrated_model.configure_for_stage2(freeze_eeg_encoder=freeze_eeg)
        
        # âœ… åˆ†æå¯è®­ç»ƒå‚æ•°
        trainable_params = [p for p in integrated_model.parameters() if p.requires_grad]
        total_trainable = sum(p.numel() for p in trainable_params)
        total_params = sum(p.numel() for p in integrated_model.parameters())
        
        print(f"ğŸ“Š æ¨¡å‹å‚æ•°ç»Ÿè®¡:")
        print(f"  æ€»å‚æ•°: {total_params:,}")
        print(f"  å¯è®­ç»ƒå‚æ•°: {total_trainable:,}")
        print(f"  å†»ç»“å‚æ•°: {total_params - total_trainable:,}")
        print(f"  è®­ç»ƒæ¯”ä¾‹: {total_trainable/total_params*100:.1f}%")
        
        # ä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡è°ƒåº¦å™¨
        optimizer = torch.optim.AdamW(trainable_params, lr=learning_rate, weight_decay=0.01)
        
        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
            optimizer, mode='min', patience=5, factor=0.5, min_lr=1e-7
        )
        
        # è®­ç»ƒå¾ªç¯
        best_train_loss = float('inf')
        patience = 15
        patience_counter = 0
        
        for epoch in range(epochs):
            print(f"\n--- Epoch {epoch+1}/{epochs} ---")
            
            # è®­ç»ƒ
            train_loss = self._train_epoch(integrated_model, train_loader, optimizer, epoch+1)
            
            scheduler.step(train_loss)
            current_lr = optimizer.param_groups[0]['lr']
            
            print(f"è®­ç»ƒæŸå¤±: {train_loss:.4f}")
            print(f"å½“å‰å­¦ä¹ ç‡: {current_lr:.2e}")
            
            # è®°å½•å†å²
            self.training_losses.append(train_loss)
            
            # æ¯5ä¸ªepochæµ‹è¯•ç”Ÿæˆæ•ˆæœ
            if (epoch + 1) % 5 == 0:
                print("\nğŸ“ ç”Ÿæˆæ ·æœ¬æµ‹è¯•:")
                self._test_generation(integrated_model, test_loader, num_samples=3)
            
            # æ—©åœå’Œä¿å­˜
            if train_loss < best_train_loss:
                best_train_loss = train_loss
                patience_counter = 0
                
                torch.save({
                    'model_state_dict': integrated_model.state_dict(),
                    'epoch': epoch,
                    'train_loss': train_loss,
                }, save_path)
                print(f"ğŸ’¾ ä¿å­˜æœ€ä½³æ¨¡å‹: {save_path}")
                
            else:
                patience_counter += 1
                if patience_counter >= patience:
                    print(f"â¹ï¸ æ—©åœè§¦å‘")
                    break
        
        print(f"\nâœ… è®­ç»ƒå®Œæˆ! æœ€ä½³è®­ç»ƒæŸå¤±: {best_train_loss:.4f}")
        return integrated_model, best_train_loss
    
    def evaluate_on_test_set(self, model, test_loader, save_dir):
        """åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°æ¨¡å‹ï¼ˆåŒ…å«BLEUåˆ†æ•°ï¼‰"""
        logger.info("\nğŸ§ª æµ‹è¯•é›†è¯„ä¼°å¼€å§‹...")
        
        model.eval()
        test_results = {
            'total_loss': 0.0,
            'num_batches': 0,
            'generated_samples': [],
            'metrics': {}
        }
        
        all_generated_texts = []
        all_original_texts = []
        all_ids = []
        
        with torch.no_grad():
            progress_bar = tqdm(test_loader, desc="æµ‹è¯•é›†è¯„ä¼°")
            
            for batch in progress_bar:
                try:
                    # è®¡ç®—æŸå¤±
                    outputs = model(
                        eeg_data=batch['eeg'],
                        text_data=batch['text'],
                        tokenizer=self.tokenizer
                    )
                    
                    loss = outputs.loss
                    test_results['total_loss'] += loss.item()
                    test_results['num_batches'] += 1
                    
                    # ç”Ÿæˆæ–‡æœ¬
                    generated_texts = model.generate_text(
                        eeg_data=batch['eeg'],
                        tokenizer=self.tokenizer,
                        max_length=32,
                        num_beams=4
                    )
                    
                    # æ”¶é›†ç»“æœ
                    for gen_text, orig_text, sample_id in zip(generated_texts, batch['text'], batch['ids']):
                        all_generated_texts.append(gen_text)
                        all_original_texts.append(orig_text)
                        all_ids.append(sample_id)
                        
                        test_results['generated_samples'].append({
                            'id': sample_id,
                            'original': orig_text,
                            'generated': gen_text
                        })
                    
                    progress_bar.set_postfix({'loss': f'{loss.item():.4f}'})
                    
                except Exception as e:
                    logger.error(f"æµ‹è¯•æ‰¹æ¬¡å¤„ç†å¤±è´¥: {e}")
                    continue
        
        # è®¡ç®—å¹³å‡æŸå¤±
        avg_test_loss = test_results['total_loss'] / test_results['num_batches'] if test_results['num_batches'] > 0 else float('inf')
        test_results['avg_loss'] = avg_test_loss
        
        # âœ… è®¡ç®—å„ç§è¯„ä¼°æŒ‡æ ‡
        logger.info("ğŸ“Š è®¡ç®—è¯„ä¼°æŒ‡æ ‡...")
        
        # 1. æ–‡æœ¬ç›¸ä¼¼åº¦ï¼ˆåŸæœ‰ï¼‰
        from difflib import SequenceMatcher
        similarities = []
        for gen, orig in zip(all_generated_texts, all_original_texts):
            sim = SequenceMatcher(None, gen, orig).ratio()
            similarities.append(sim)
        
        # 2. âœ… BLEUåˆ†æ•°è®¡ç®—
        logger.info("ğŸ”¢ è®¡ç®—BLEUåˆ†æ•°...")
        bleu_scores = {
            'bleu_1': [],
            'bleu_2': [],
            'bleu_3': [],
            'bleu_4': [],
            'corpus_bleu_1': 0.0,
            'corpus_bleu_2': 0.0,
            'corpus_bleu_3': 0.0,
            'corpus_bleu_4': 0.0
        }
        
        # è®¡ç®—æ¯ä¸ªå¥å­çš„BLEUåˆ†æ•°
        for gen_text, orig_text in zip(all_generated_texts, all_original_texts):
            # BLEU-1åˆ°BLEU-4
            bleu_1 = self.bleu_calculator.calculate_sentence_bleu(orig_text, gen_text, weights=(1.0, 0, 0, 0))
            bleu_2 = self.bleu_calculator.calculate_sentence_bleu(orig_text, gen_text, weights=(0.5, 0.5, 0, 0))
            bleu_3 = self.bleu_calculator.calculate_sentence_bleu(orig_text, gen_text, weights=(0.33, 0.33, 0.33, 0))
            bleu_4 = self.bleu_calculator.calculate_sentence_bleu(orig_text, gen_text, weights=(0.25, 0.25, 0.25, 0.25))
            
            bleu_scores['bleu_1'].append(bleu_1)
            bleu_scores['bleu_2'].append(bleu_2)
            bleu_scores['bleu_3'].append(bleu_3)
            bleu_scores['bleu_4'].append(bleu_4)
        
        # è®¡ç®—è¯­æ–™åº“çº§BLEUåˆ†æ•°
        if all_generated_texts and all_original_texts:
            bleu_scores['corpus_bleu_1'] = self.bleu_calculator.calculate_corpus_bleu(
                all_original_texts, all_generated_texts, weights=(1.0, 0, 0, 0)
            )
            bleu_scores['corpus_bleu_2'] = self.bleu_calculator.calculate_corpus_bleu(
                all_original_texts, all_generated_texts, weights=(0.5, 0.5, 0, 0)
            )
            bleu_scores['corpus_bleu_3'] = self.bleu_calculator.calculate_corpus_bleu(
                all_original_texts, all_generated_texts, weights=(0.33, 0.33, 0.33, 0)
            )
            bleu_scores['corpus_bleu_4'] = self.bleu_calculator.calculate_corpus_bleu(
                all_original_texts, all_generated_texts, weights=(0.25, 0.25, 0.25, 0.25)
            )
        
        # 3. âœ… ROUGE-likeåˆ†æ•°
        def calculate_rouge_like_score(reference, candidate):
            """ç®€å•çš„ROUGE-Lç±»ä¼¼åˆ†æ•°"""
            ref_tokens = set(self.bleu_calculator._tokenize_chinese(reference))
            cand_tokens = set(self.bleu_calculator._tokenize_chinese(candidate))
            
            if not ref_tokens or not cand_tokens:
                return 0.0
            
            intersection = ref_tokens.intersection(cand_tokens)
            if not intersection:
                return 0.0
            
            precision = len(intersection) / len(cand_tokens)
            recall = len(intersection) / len(ref_tokens)
            
            if precision + recall == 0:
                return 0.0
            
            f1 = 2 * precision * recall / (precision + recall)
            return f1
        
        rouge_like_scores = []
        for gen, orig in zip(all_generated_texts, all_original_texts):
            score = calculate_rouge_like_score(orig, gen)
            rouge_like_scores.append(score)
        
        # æ•´åˆæ‰€æœ‰æŒ‡æ ‡
        if similarities:
            test_results['metrics'] = {
                # åŸºæœ¬æ–‡æœ¬ç›¸ä¼¼åº¦
                'avg_similarity': np.mean(similarities),
                'median_similarity': np.median(similarities),
                'std_similarity': np.std(similarities),
                'max_similarity': np.max(similarities),
                'min_similarity': np.min(similarities),
                
                # âœ… BLEUåˆ†æ•°
                'bleu_scores': {
                    'avg_bleu_1': np.mean(bleu_scores['bleu_1']) if bleu_scores['bleu_1'] else 0.0,
                    'avg_bleu_2': np.mean(bleu_scores['bleu_2']) if bleu_scores['bleu_2'] else 0.0,
                    'avg_bleu_3': np.mean(bleu_scores['bleu_3']) if bleu_scores['bleu_3'] else 0.0,
                    'avg_bleu_4': np.mean(bleu_scores['bleu_4']) if bleu_scores['bleu_4'] else 0.0,
                    'corpus_bleu_1': bleu_scores['corpus_bleu_1'],
                    'corpus_bleu_2': bleu_scores['corpus_bleu_2'],
                    'corpus_bleu_3': bleu_scores['corpus_bleu_3'],
                    'corpus_bleu_4': bleu_scores['corpus_bleu_4'],
                    'std_bleu_1': np.std(bleu_scores['bleu_1']) if bleu_scores['bleu_1'] else 0.0,
                    'std_bleu_2': np.std(bleu_scores['bleu_2']) if bleu_scores['bleu_2'] else 0.0,
                    'std_bleu_3': np.std(bleu_scores['bleu_3']) if bleu_scores['bleu_3'] else 0.0,
                    'std_bleu_4': np.std(bleu_scores['bleu_4']) if bleu_scores['bleu_4'] else 0.0,
                },
                
                # âœ… ROUGE-likeåˆ†æ•°
                'rouge_like': {
                    'avg_rouge_like': np.mean(rouge_like_scores) if rouge_like_scores else 0.0,
                    'std_rouge_like': np.std(rouge_like_scores) if rouge_like_scores else 0.0,
                    'max_rouge_like': np.max(rouge_like_scores) if rouge_like_scores else 0.0,
                    'min_rouge_like': np.min(rouge_like_scores) if rouge_like_scores else 0.0,
                }
            }
        
        # ä¿å­˜æµ‹è¯•ç»“æœ
        test_results_path = save_dir / 'test_results_with_bleu.json'
        with open(test_results_path, 'w', encoding='utf-8') as f:
            json.dump(test_results, f, ensure_ascii=False, indent=2)
        
        # âœ… ä¿å­˜è¯¦ç»†çš„è¯„ä¼°ç»“æœï¼ˆåŒ…å«BLEUåˆ†æ•°ï¼‰
        detailed_results = []
        for i, (gen_text, orig_text, sample_id) in enumerate(zip(all_generated_texts, all_original_texts, all_ids)):
            detailed_results.append({
                'id': sample_id,
                'original_text': orig_text,
                'generated_text': gen_text,
                'similarity': similarities[i] if similarities else 0,
                'bleu_1': bleu_scores['bleu_1'][i] if i < len(bleu_scores['bleu_1']) else 0,
                'bleu_2': bleu_scores['bleu_2'][i] if i < len(bleu_scores['bleu_2']) else 0,
                'bleu_3': bleu_scores['bleu_3'][i] if i < len(bleu_scores['bleu_3']) else 0,
                'bleu_4': bleu_scores['bleu_4'][i] if i < len(bleu_scores['bleu_4']) else 0,
                'rouge_like': rouge_like_scores[i] if i < len(rouge_like_scores) else 0,
            })
        
        samples_df = pd.DataFrame(detailed_results)
        samples_df.to_csv(save_dir / 'test_samples_with_bleu.csv', index=False, encoding='utf-8')
        
        # âœ… æ‰“å°è¯¦ç»†çš„æµ‹è¯•ç»“æœ
        logger.info(f"\nğŸ“Š æµ‹è¯•é›†è¯„ä¼°ç»“æœ:")
        logger.info(f"  ğŸ“ å¹³å‡ç”ŸæˆæŸå¤±: {avg_test_loss:.4f}")
        logger.info(f"  ğŸ“Š æµ‹è¯•æ ·æœ¬æ•°é‡: {len(all_generated_texts)}")
        
        if similarities:
            logger.info(f"\n  ğŸ“ˆ æ–‡æœ¬ç›¸ä¼¼åº¦æŒ‡æ ‡:")
            logger.info(f"    å¹³å‡ç›¸ä¼¼åº¦: {test_results['metrics']['avg_similarity']:.4f}")
            logger.info(f"    ä¸­ä½æ•°ç›¸ä¼¼åº¦: {test_results['metrics']['median_similarity']:.4f}")
            logger.info(f"    æœ€é«˜ç›¸ä¼¼åº¦: {test_results['metrics']['max_similarity']:.4f}")
            logger.info(f"    æœ€ä½ç›¸ä¼¼åº¦: {test_results['metrics']['min_similarity']:.4f}")
            
            # âœ… BLEUåˆ†æ•°
            logger.info(f"\n  ğŸ”¢ BLEUåˆ†æ•°:")
            bleu_metrics = test_results['metrics']['bleu_scores']
            logger.info(f"    å¹³å‡ BLEU-1: {bleu_metrics['avg_bleu_1']:.4f} Â± {bleu_metrics['std_bleu_1']:.4f}")
            logger.info(f"    å¹³å‡ BLEU-2: {bleu_metrics['avg_bleu_2']:.4f} Â± {bleu_metrics['std_bleu_2']:.4f}")
            logger.info(f"    å¹³å‡ BLEU-3: {bleu_metrics['avg_bleu_3']:.4f} Â± {bleu_metrics['std_bleu_3']:.4f}")
            logger.info(f"    å¹³å‡ BLEU-4: {bleu_metrics['avg_bleu_4']:.4f} Â± {bleu_metrics['std_bleu_4']:.4f}")
            
            logger.info(f"\n  ğŸ“š è¯­æ–™åº“çº§ BLEUåˆ†æ•°:")
            logger.info(f"    è¯­æ–™åº“ BLEU-1: {bleu_metrics['corpus_bleu_1']:.4f}")
            logger.info(f"    è¯­æ–™åº“ BLEU-2: {bleu_metrics['corpus_bleu_2']:.4f}")
            logger.info(f"    è¯­æ–™åº“ BLEU-3: {bleu_metrics['corpus_bleu_3']:.4f}")
            logger.info(f"    è¯­æ–™åº“ BLEU-4: {bleu_metrics['corpus_bleu_4']:.4f}")
            
            # âœ… ROUGE-likeåˆ†æ•°
            rouge_metrics = test_results['metrics']['rouge_like']
            logger.info(f"\n  ğŸ“‹ ROUGE-likeåˆ†æ•°:")
            logger.info(f"    å¹³å‡ ROUGE-like: {rouge_metrics['avg_rouge_like']:.4f} Â± {rouge_metrics['std_rouge_like']:.4f}")
            logger.info(f"    æœ€é«˜ ROUGE-like: {rouge_metrics['max_rouge_like']:.4f}")
            logger.info(f"    æœ€ä½ ROUGE-like: {rouge_metrics['min_rouge_like']:.4f}")
        
        # æ˜¾ç¤ºä¸€äº›ç”Ÿæˆæ ·æœ¬
        logger.info(f"\nğŸ“ æµ‹è¯•é›†ç”Ÿæˆæ ·æœ¬ç¤ºä¾‹ (å¸¦BLEUåˆ†æ•°):")
        for i, sample in enumerate(test_results['generated_samples'][:5]):
            detail = detailed_results[i]
            logger.info(f"  æ ·æœ¬ {i+1} (ID: {sample['id']}):")
            logger.info(f"    åŸæ–‡: {sample['original']}")
            logger.info(f"    ç”Ÿæˆ: {sample['generated']}")
            logger.info(f"    ç›¸ä¼¼åº¦: {detail['similarity']:.3f}")
            logger.info(f"    BLEU-1: {detail['bleu_1']:.3f}, BLEU-2: {detail['bleu_2']:.3f}, BLEU-3: {detail['bleu_3']:.3f}, BLEU-4: {detail['bleu_4']:.3f}")
            logger.info(f"    ROUGE-like: {detail['rouge_like']:.3f}")
        
        logger.info(f"\nğŸ“ æµ‹è¯•ç»“æœä¿å­˜åˆ°: {test_results_path}")
        logger.info(f"ğŸ“ è¯¦ç»†æ ·æœ¬ï¼ˆå«BLEUï¼‰ä¿å­˜åˆ°: {save_dir / 'test_samples_with_bleu.csv'}")
        
        return test_results
    
    def _train_epoch(self, model, train_loader, optimizer, epoch):
        """è®­ç»ƒepoch"""
        model.train()
        total_loss = 0.0
        num_batches = 0
        
        progress_bar = tqdm(train_loader, desc=f"Train Epoch {epoch}")
        
        for batch in progress_bar:
            optimizer.zero_grad()
            
            try:
                outputs = model(
                    eeg_data=batch['eeg'],
                    text_data=batch['text'],
                    tokenizer=self.tokenizer
                )
                
                loss = outputs.loss
                
                loss.backward()
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
                optimizer.step()
                
                total_loss += loss.item()
                num_batches += 1
                
                progress_bar.set_postfix({'loss': f'{loss.item():.4f}'})
                
            except Exception as e:
                logger.error(f"è®­ç»ƒæ‰¹æ¬¡å‡ºé”™: {e}")
                continue
        
        return total_loss / num_batches if num_batches > 0 else float('inf')
    
    def _test_generation(self, model, test_loader, num_samples=3):
        """æµ‹è¯•ç”Ÿæˆæ•ˆæœ"""
        model.eval()
        samples_generated = 0
        
        with torch.no_grad():
            for batch in test_loader:
                if samples_generated >= num_samples:
                    break
                
                eeg_data = batch['eeg'][:min(num_samples - samples_generated, len(batch['eeg']))]
                original_texts = batch['text'][:len(eeg_data)]
                ids = batch['ids'][:len(eeg_data)]
                
                try:
                    generated_texts = model.generate_text(
                        eeg_data=eeg_data,
                        tokenizer=self.tokenizer,
                        max_length=32,
                        num_beams=4
                    )
                    
                    for i, (gen_text, orig_text, sample_id) in enumerate(zip(generated_texts, original_texts, ids)):
                        print(f"  æ ·æœ¬ {samples_generated + i + 1} (ID: {sample_id}):")
                        print(f"    åŸæ–‡: {orig_text}")
                        print(f"    ç”Ÿæˆ: {gen_text}")
                        
                    samples_generated += len(generated_texts)
                    
                except Exception as e:
                    print(f"  ç”Ÿæˆå¤±è´¥: {e}")
                    break

def main():
    """ä¸»è®­ç»ƒå‡½æ•°"""
    config = {
        'csv_file': 'additional_data/subject1_gxy_0328/gxy_0328.csv',
        'eeg_dir': 'additional_data/subject1_gxy_0328/',
        'batch_size': 16,
        'test_size': 0.2,      # âœ… æµ‹è¯•é›†æ¯”ä¾‹
        'random_state': 42,
        
        # è®­ç»ƒå‚æ•°
        'epochs': 60,
        'learning_rate': 2e-5,
        'freeze_eeg': True,  # å†»ç»“EEGç¼–ç å™¨
        
        # é¢„è®­ç»ƒæ¨¡å‹è·¯å¾„
        'pretrained_model_path': 'stage1_only_900samples_20250711_164810/stage1_contrastive_pretrained.pth',
        
        'save_dir': f'train_test_bleu_{datetime.now().strftime("%Y%m%d_%H%M%S")}'
    }
    
    logger.info("=== EEG-to-Textè®­ç»ƒï¼ˆå«BLEUè¯„ä¼°ï¼‰ ===")
    logger.info("ğŸ¯ è®­ç»ƒç­–ç•¥:")
    logger.info("  ğŸ“š ä½¿ç”¨300åˆ†ç±»é¢„è®­ç»ƒçš„EEGç¼–ç å™¨")
    logger.info("  ğŸ”’ å†»ç»“EEGç¼–ç å™¨ï¼Œå¾®è°ƒBARTè§£ç å™¨")
    logger.info("  ğŸ“Š äºŒåˆ†å‰²: è®­ç»ƒé›†/æµ‹è¯•é›†")
    logger.info("  ğŸ”¢ åŒ…å«BLEUåˆ†æ•°è¯„ä¼°")
    
    # æ£€æŸ¥æ•°æ®æ–‡ä»¶
    if not os.path.exists(config['csv_file']):
        logger.error(f"CSVæ–‡ä»¶ä¸å­˜åœ¨: {config['csv_file']}")
        return
    
    if not os.path.exists(config['eeg_dir']):
        logger.error(f"EEGç›®å½•ä¸å­˜åœ¨: {config['eeg_dir']}")
        return
    
    if not os.path.exists(config['pretrained_model_path']):
        logger.error(f"é¢„è®­ç»ƒæ¨¡å‹ä¸å­˜åœ¨: {config['pretrained_model_path']}")
        return
    
    # åˆ›å»ºä¿å­˜ç›®å½•
    save_dir = Path(config['save_dir'])
    save_dir.mkdir(parents=True, exist_ok=True)
    
    # åˆå§‹åŒ–tokenizer
    logger.info("åˆå§‹åŒ–tokenizer...")
    try:
        from transformers import AutoTokenizer
        tokenizer = AutoTokenizer.from_pretrained('fnlp/bart-base-chinese')
        logger.info(f"âœ… æˆåŠŸåŠ è½½tokenizer: {type(tokenizer).__name__}")
    except:
        from transformers import BertTokenizer
        tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')
        logger.info("âœ… ä½¿ç”¨BertTokenizer")
    
    # åˆ›å»ºæ•°æ®é›†
    logger.info("åˆ›å»ºæ•°æ®é›†...")
    full_dataset = EEGTextDataset(
        csv_file=config['csv_file'],
        eeg_dir=config['eeg_dir'],
        tokenizer=tokenizer
    )
    
    if len(full_dataset) == 0:
        logger.error("âŒ æ•°æ®é›†ä¸ºç©ºï¼Œæ— æ³•è®­ç»ƒ!")
        return
    
    # âœ… åˆ›å»ºè®­ç»ƒ/æµ‹è¯•é›†åˆ†å‰²
    train_indices, test_indices = create_train_test_split(
        full_dataset,
        test_size=config['test_size'],
        random_state=config['random_state']
    )
    
    # ä¿å­˜æ•°æ®åˆ†å‰²ç´¢å¼•
    save_data_splits(train_indices, test_indices, save_dir)
    
    # åˆ›å»ºæ•°æ®å­é›†
    train_dataset = Subset(full_dataset, train_indices)
    test_dataset = Subset(full_dataset, test_indices)
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_loader = DataLoader(
        train_dataset, batch_size=config['batch_size'], 
        shuffle=True, collate_fn=collate_fn, num_workers=0
    )
    
    test_loader = DataLoader(
        test_dataset, batch_size=config['batch_size'], 
        shuffle=False, collate_fn=collate_fn, num_workers=0
    )
    
    # åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
    logger.info(f"åŠ è½½é¢„è®­ç»ƒçš„300åˆ†ç±»æ¨¡å‹: {config['pretrained_model_path']}")
    pretrained_model = ContrastiveEEGTextModel(
        patch_dim=256,
        feature_dim=768,
        target_channels=66,
        target_length=4000
    )
    
    checkpoint = torch.load(config['pretrained_model_path'], map_location='cpu', weights_only=False)
    pretrained_model.load_state_dict(checkpoint['model_state_dict'])
    logger.info(f"âœ… æˆåŠŸåŠ è½½é¢„è®­ç»ƒæ¨¡å‹ (epoch {checkpoint.get('epoch', 'unknown')})")
    
    # åˆ›å»ºè®­ç»ƒå™¨
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    logger.info(f"ä½¿ç”¨è®¾å¤‡: {device}")
    pretrained_model = pretrained_model.to(device)
    trainer = IntegratedTrainer(tokenizer, device)
    
    # è®­ç»ƒæ¨¡å‹
    model_path = save_dir / 'final_model.pth'
    final_model, best_loss = trainer.train_with_test_evaluation(
        train_loader=train_loader,
        test_loader=test_loader,
        pretrained_model=pretrained_model,
        epochs=config['epochs'],
        learning_rate=config['learning_rate'],
        save_path=model_path,
        freeze_eeg=config['freeze_eeg']
    )
    
    # âœ… åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°æœ€ç»ˆæ¨¡å‹
    logger.info("\n" + "="*80)
    logger.info("ğŸ§ª æµ‹è¯•é›†è¯„ä¼°ï¼ˆåŒ…å«BLEUåˆ†æ•°ï¼‰")
    logger.info("="*80)
    
    # åŠ è½½æœ€ä½³æ¨¡å‹
    best_checkpoint = torch.load(model_path, map_location=device)
    final_model.load_state_dict(best_checkpoint['model_state_dict'])
    
    # æµ‹è¯•é›†è¯„ä¼°
    test_results = trainer.evaluate_on_test_set(final_model, test_loader, save_dir)
    
    # âœ… ç»˜åˆ¶åŒ…å«BLEUåˆ†æ•°çš„è®­ç»ƒå†å²å’Œç»“æœ
    if trainer.training_losses:
        plt.figure(figsize=(15, 10))
        
        # è®­ç»ƒæŸå¤±æ›²çº¿
        plt.subplot(2, 3, 1)
        epochs = range(1, len(trainer.training_losses) + 1)
        plt.plot(epochs, trainer.training_losses, 'b-', label='è®­ç»ƒæŸå¤±', linewidth=2)
        plt.axhline(y=test_results['avg_loss'], color='r', linestyle='--', label=f'æµ‹è¯•æŸå¤± ({test_results["avg_loss"]:.4f})')
        plt.xlabel('Epoch')
        plt.ylabel('æŸå¤±')
        plt.title('è®­ç»ƒæŸå¤± vs æµ‹è¯•æŸå¤±')
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        # âœ… BLEUåˆ†æ•°æ¡å½¢å›¾
        plt.subplot(2, 3, 2)
        if 'bleu_scores' in test_results['metrics']:
            bleu_metrics = test_results['metrics']['bleu_scores']
            bleu_names = ['BLEU-1', 'BLEU-2', 'BLEU-3', 'BLEU-4']
            bleu_values = [
                bleu_metrics['avg_bleu_1'],
                bleu_metrics['avg_bleu_2'],
                bleu_metrics['avg_bleu_3'],
                bleu_metrics['avg_bleu_4']
            ]
            bleu_stds = [
                bleu_metrics['std_bleu_1'],
                bleu_metrics['std_bleu_2'],
                bleu_metrics['std_bleu_3'],
                bleu_metrics['std_bleu_4']
            ]
            
            plt.bar(bleu_names, bleu_values, yerr=bleu_stds, capsize=5, 
                   color=['skyblue', 'lightgreen', 'lightcoral', 'gold'], alpha=0.8)
            plt.ylabel('BLEUåˆ†æ•°')
            plt.title('æµ‹è¯•é›† BLEUåˆ†æ•°')
            plt.grid(True, alpha=0.3)
        
        # âœ… è¯­æ–™åº“çº§BLEUåˆ†æ•°
        plt.subplot(2, 3, 3)
        if 'bleu_scores' in test_results['metrics']:
            corpus_bleu_names = ['Corpus\nBLEU-1', 'Corpus\nBLEU-2', 'Corpus\nBLEU-3', 'Corpus\nBLEU-4']
            corpus_bleu_values = [
                bleu_metrics['corpus_bleu_1'],
                bleu_metrics['corpus_bleu_2'],
                bleu_metrics['corpus_bleu_3'],
                bleu_metrics['corpus_bleu_4']
            ]
            
            plt.bar(corpus_bleu_names, corpus_bleu_values, 
                   color=['navy', 'darkgreen', 'darkred', 'orange'], alpha=0.8)
            plt.ylabel('è¯­æ–™åº“BLEUåˆ†æ•°')
            plt.title('è¯­æ–™åº“çº§ BLEUåˆ†æ•°')
            plt.grid(True, alpha=0.3)
        
        # âœ… å„ç§æŒ‡æ ‡å¯¹æ¯”
        plt.subplot(2, 3, 4)
        if 'avg_similarity' in test_results['metrics'] and 'bleu_scores' in test_results['metrics']:
            metrics_names = ['æ–‡æœ¬\nç›¸ä¼¼åº¦', 'BLEU-1', 'BLEU-2', 'BLEU-4', 'ROUGE\n-like']
            metrics_values = [
                test_results['metrics']['avg_similarity'],
                test_results['metrics']['bleu_scores']['avg_bleu_1'],
                test_results['metrics']['bleu_scores']['avg_bleu_2'],
                test_results['metrics']['bleu_scores']['avg_bleu_4'],
                test_results['metrics']['rouge_like']['avg_rouge_like']
            ]
            
            plt.bar(metrics_names, metrics_values, 
                   color=['purple', 'skyblue', 'lightgreen', 'gold', 'pink'], alpha=0.8)
            plt.ylabel('åˆ†æ•°')
            plt.title('å„è¯„ä¼°æŒ‡æ ‡å¯¹æ¯”')
            plt.grid(True, alpha=0.3)
        
        # âœ… æ€§èƒ½æ€»ç»“é›·è¾¾å›¾ï¼ˆç®€åŒ–ç‰ˆï¼‰
        plt.subplot(2, 3, 5)
        if 'bleu_scores' in test_results['metrics']:
            performance_summary = {
                'æŸå¤±': 1 - min(1.0, test_results['avg_loss'] / 10),  # å½’ä¸€åŒ–æŸå¤±
                'BLEU-1': test_results['metrics']['bleu_scores']['avg_bleu_1'],
                'BLEU-4': test_results['metrics']['bleu_scores']['avg_bleu_4'],
                'ç›¸ä¼¼åº¦': test_results['metrics']['avg_similarity'],
                'ROUGE': test_results['metrics']['rouge_like']['avg_rouge_like']
            }
            
            metrics_names = list(performance_summary.keys())
            metrics_values = list(performance_summary.values())
            
            plt.bar(metrics_names, metrics_values, 
                   color='lightblue', alpha=0.8, edgecolor='navy', linewidth=1)
            plt.ylabel('æ€§èƒ½åˆ†æ•°')
            plt.title('ç»¼åˆæ€§èƒ½æ€»ç»“')
            plt.ylim(0, 1)
            plt.grid(True, alpha=0.3)
            
            # æ·»åŠ æ•°å€¼æ ‡ç­¾
            for i, v in enumerate(metrics_values):
                plt.text(i, v + 0.01, f'{v:.3f}', ha='center', va='bottom')
        
        # âœ… è®­ç»ƒè¿›åº¦å’Œæœ€ç»ˆç»“æœ
        plt.subplot(2, 3, 6)
        final_metrics = [
            f"è®­ç»ƒè½®æ•°: {len(trainer.training_losses)}",
            f"æœ€ä½³æŸå¤±: {best_loss:.4f}",
            f"æµ‹è¯•æŸå¤±: {test_results['avg_loss']:.4f}",
            f"BLEU-4: {test_results['metrics'].get('bleu_scores', {}).get('avg_bleu_4', 0):.3f}",
            f"ç›¸ä¼¼åº¦: {test_results['metrics'].get('avg_similarity', 0):.3f}"
        ]
        
        plt.text(0.1, 0.8, "ğŸ¯ è®­ç»ƒç»“æœæ€»ç»“", fontsize=14, fontweight='bold')
        for i, metric in enumerate(final_metrics):
            plt.text(0.1, 0.7 - i*0.1, metric, fontsize=10)
        plt.xlim(0, 1)
        plt.ylim(0, 1)
        plt.axis('off')
        
        plt.tight_layout()
        plot_path = save_dir / 'training_results_with_bleu.png'
        plt.savefig(plot_path, dpi=300, bbox_inches='tight')
        plt.show()
        
        logger.info(f"è®­ç»ƒå†å²å›¾ï¼ˆå«BLEUï¼‰ä¿å­˜åˆ°: {plot_path}")
    
    # âœ… ä¿å­˜åŒ…å«BLEUåˆ†æ•°çš„å®Œæ•´ç»“æœ
    results = {
        'best_train_loss': best_loss,
        'test_results': test_results,
        'training_history': {
            'losses': trainer.training_losses
        },
        'data_splits': {
            'train_size': len(train_indices),
            'test_size': len(test_indices),
            'train_ratio': len(train_indices) / len(full_dataset),
            'test_ratio': len(test_indices) / len(full_dataset)
        },
        'config': config,
        'training_strategy': 'pretrained_eeg_encoder_with_bart_decoder',
        'pretrained_model_info': {
            'path': config['pretrained_model_path'],
            'epoch': checkpoint.get('epoch', 'unknown'),
            'val_accuracy': checkpoint.get('val_accuracy', 'unknown')
        },
        'timestamp': datetime.now().isoformat(),
        # âœ… è¯„ä¼°æ‘˜è¦
        'evaluation_summary': {
            'avg_loss': test_results['avg_loss'],
            'avg_similarity': test_results['metrics'].get('avg_similarity', 0),
            'avg_bleu_1': test_results['metrics'].get('bleu_scores', {}).get('avg_bleu_1', 0),
            'avg_bleu_2': test_results['metrics'].get('bleu_scores', {}).get('avg_bleu_2', 0),
            'avg_bleu_3': test_results['metrics'].get('bleu_scores', {}).get('avg_bleu_3', 0),
            'avg_bleu_4': test_results['metrics'].get('bleu_scores', {}).get('avg_bleu_4', 0),
            'corpus_bleu_4': test_results['metrics'].get('bleu_scores', {}).get('corpus_bleu_4', 0),
            'avg_rouge_like': test_results['metrics'].get('rouge_like', {}).get('avg_rouge_like', 0)
        }
    }
    
    results_path = save_dir / 'final_results_with_bleu.json'
    with open(results_path, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    
    # âœ… å®Œæˆä¿¡æ¯ï¼ˆåŒ…å«BLEUåˆ†æ•°ï¼‰
    logger.info(f"\nğŸ‰ EEG-to-Textè®­ç»ƒå®Œæˆ!")
    logger.info(f"ğŸ“Š æœ€ä½³è®­ç»ƒæŸå¤±: {best_loss:.4f}")
    logger.info(f"ğŸ“Š æµ‹è¯•é›†å¹³å‡æŸå¤±: {test_results['avg_loss']:.4f}")
    
    if 'bleu_scores' in test_results['metrics']:
        bleu_scores = test_results['metrics']['bleu_scores']
        logger.info(f"ğŸ“Š æµ‹è¯•é›†BLEUåˆ†æ•°:")
        logger.info(f"   BLEU-1: {bleu_scores['avg_bleu_1']:.4f}")
        logger.info(f"   BLEU-2: {bleu_scores['avg_bleu_2']:.4f}")
        logger.info(f"   BLEU-3: {bleu_scores['avg_bleu_3']:.4f}")
        logger.info(f"   BLEU-4: {bleu_scores['avg_bleu_4']:.4f}")
        logger.info(f"   è¯­æ–™åº“BLEU-4: {bleu_scores['corpus_bleu_4']:.4f}")
    
    if 'avg_similarity' in test_results['metrics']:
        logger.info(f"ğŸ“Š æµ‹è¯•é›†å¹³å‡ç›¸ä¼¼åº¦: {test_results['metrics']['avg_similarity']:.4f}")
    
    if 'rouge_like' in test_results['metrics']:
        logger.info(f"ğŸ“Š æµ‹è¯•é›†ROUGE-like: {test_results['metrics']['rouge_like']['avg_rouge_like']:.4f}")
    
    logger.info(f"ğŸ“‚ ç»“æœä¿å­˜åœ¨: {save_dir}")
    logger.info(f"ğŸ’¾ æœ€ç»ˆæ¨¡å‹: {model_path}")
    logger.info(f"ğŸ“Š æµ‹è¯•ç»“æœ: {save_dir / 'test_results_with_bleu.json'}")
    logger.info(f"ğŸ“ ç”Ÿæˆæ ·æœ¬: {save_dir / 'test_samples_with_bleu.csv'}")

if __name__ == "__main__":
    main()
